# MLX Speculative Server

A high-performance speculative decoding server for Apple Silicon, providing vLLM-like capabilities with support for concurrent requests and multiple models.

## üöÄ Features

- **Speculative Decoding**: 2-4x throughput improvement through draft model speculation
- **High Throughput**: Target performance of 500-1000 tokens/second on Apple Silicon
- **Multi-Model Support**: Serve multiple models simultaneously with intelligent load balancing
- **Concurrent Requests**: Handle hundreds of concurrent requests efficiently
- **Model Groups**: Organize related models into logical groups
- **Auto-Draft Models**: Automatically create draft models or use custom ones
- **Memory Management**: Intelligent model loading/unloading based on usage patterns
- **RESTful API**: FastAPI-based server with comprehensive endpoints
- **Real-time Metrics**: Performance tracking and optimization recommendations

## üìã Requirements

- **Apple Silicon Mac** (M1, M2, M3, or later)
- **Python 3.8+**
- **MLX Framework**
- **8GB+ RAM** (16GB+ recommended for multiple models)

## üõ† Installation

### From Source

```bash
git clone https://github.com/yourusername/mlx-speculative-server.git
cd mlx-speculative-server
pip install -e .
```

### Dependencies

```bash
pip install mlx mlx-lm fastapi uvicorn numpy transformers
```

## üöÄ Quick Start

### 1. Download and Start with a Model

```bash
# Download a model from Hugging Face
mlx-spec download microsoft/Phi-3-mini-4k-instruct

# Start server with the downloaded model
mlx-spec serve --model phi3-mini --model-path microsoft/Phi-3-mini-4k-instruct

# Or start with a local model path
mlx-spec serve --model llama3-8b --model-path /path/to/llama3-8b

# Start with configuration file
mlx-spec serve --config config.json
```

### 1.1. Model Management

```bash
# Search for models on Hugging Face
mlx-spec search "phi-3"

# List popular models
mlx-spec popular

# List locally downloaded models
mlx-spec list

# Get model information
mlx-spec info microsoft/Phi-3-mini-4k-instruct

# Download with quantization
mlx-spec download microsoft/Phi-3-mini-4k-instruct --quantize q4
```

### 2. Generate Text

```bash
curl -X POST "http://localhost:8000/generate" \
     -H "Content-Type: application/json" \
     -d '{
       "prompt": "What is the future of AI?",
       "max_tokens": 100,
       "temperature": 0.7
     }'
```

### 3. Load Additional Models

```bash
curl -X POST "http://localhost:8000/models/load" \
     -H "Content-Type: application/json" \
     -d '{
       "name": "phi3-mini",
       "model_path": "/path/to/phi3-mini",
       "auto_draft": true
     }'
```

## üìñ API Documentation

### Generate Text

**POST** `/generate`

```json
{
  "prompt": "Your prompt here",
  "model": "model_name",
  "max_tokens": 100,
  "temperature": 0.7,
  "top_p": 0.9,
  "stream": false
}
```

### Batch Generation

**POST** `/batch_generate`

```json
{
  "prompts": ["Prompt 1", "Prompt 2", "Prompt 3"],
  "model": "model_name",
  "max_tokens": 50,
  "temperature": 0.7
}
```

### Model Management

```bash
# List models
GET /models

# Load model (local or HF)
POST /models/load

# Unload model
DELETE /models/{model_name}

# Get model info
GET /models/{model_name}
```

### Hugging Face Integration

```bash
# Search HF models
POST /hf/models/search

# Download HF model
POST /hf/models/download

# List local HF models
GET /hf/models/local

# Get HF model info
GET /hf/models/{model_id}/info

# Delete local HF model
DELETE /hf/models/{model_id}

# Get popular models
GET /hf/models/popular

# Cache statistics
GET /hf/cache/stats
```

### Model Groups

```bash
# Create model group
POST /groups

# Search models
POST /models/search
```

### Server Statistics

```bash
# Get server stats
GET /stats

# Get optimization recommendations
GET /optimize

# Health check
GET /health
```

## ‚öôÔ∏è Configuration

### Configuration File Example

```json
{
  "server": {
    "host": "0.0.0.0",
    "port": 8000,
    "max_batch_size": 8,
    "batch_timeout": 0.1,
    "max_concurrent": 100
  },
  "models": {
    "llama3-8b": "/path/to/llama3-8b",
    "llama3-8b-instruct": "/path/to/llama3-8b-instruct",
    "phi3-mini": "/path/to/phi3-mini"
  },
  "groups": {
    "llama-family": {
      "models": ["llama3-8b", "llama3-8b-instruct"],
      "default_model": "llama3-8b-instruct",
      "description": "Llama 3 model family"
    }
  },
  "generation": {
    "auto_draft": true,
    "draft_layers_ratio": 0.5,
    "trust_remote_code": false
  }
}
```

### Environment Variables

```bash
# Model paths
export MODEL_PATH="/path/to/models"
export DRAFT_MODEL_PATH="/path/to/draft/models"

# Server settings
export MAX_BATCH_SIZE=8
export MAX_CONCURRENT_REQUESTS=100
export MEMORY_LIMIT_GB=16

# Performance tuning
export AUTO_DRAFT=true
export DRAFT_LAYERS_RATIO=0.5
```

## üèó Architecture

### Core Components

1. **SpeculativeEngine**: Core speculative decoding implementation
2. **MultiModelManager**: Advanced model management with load balancing
3. **BatchedKVCache**: Efficient batched key-value caching
4. **EnhancedServer**: FastAPI-based server with concurrent request handling

### Speculative Decoding Flow

```
1. Draft Model generates candidate tokens
2. Target Model verifies candidates in parallel
3. Accept/reject tokens based on probability ratios
4. Continue generation with accepted tokens
```

### Performance Optimizations

- **Batched Processing**: Process multiple requests simultaneously
- **KV Cache Management**: Efficient memory usage with sliding windows
- **Load Balancing**: Distribute requests across model instances
- **Auto-scaling**: Dynamic model loading/unloading

## üìä Performance

### Benchmarks

| Model Size | Batch Size | Throughput (tok/s) | Acceptance Rate | Memory (GB) |
|------------|------------|-------------------|-----------------|-------------|
| 7B         | 1          | 150-200           | 65-75%          | 4-6         |
| 7B         | 8          | 800-1200          | 65-75%          | 6-8         |
| 13B        | 1          | 100-150           | 70-80%          | 8-12        |
| 13B        | 4          | 400-600           | 70-80%          | 10-14       |

### Optimization Tips

1. **Use appropriate batch sizes** for your hardware
2. **Enable auto-draft models** for best performance
3. **Adjust draft_layers_ratio** based on accuracy needs
4. **Monitor memory usage** and set appropriate limits
5. **Use model groups** for related models

## üß™ Testing

### Run Tests

```bash
# Run all tests
python run_tests.py

# Run specific test categories
python run_tests.py --unit
python run_tests.py --integration
python run_tests.py --performance

# Quick tests (skip slow ones)
python run_tests.py --quick

# With coverage
python run_tests.py --coverage
```

### Test Categories

- **Unit Tests**: Individual component testing
- **Integration Tests**: Server and API testing
- **Performance Tests**: Benchmarks and stress tests

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development Setup

```bash
# Clone repository
git clone https://github.com/yourusername/mlx-speculative-server.git
cd mlx-speculative-server

# Install in development mode
pip install -e ".[dev]"

# Run tests
python run_tests.py

# Run linting
flake8 mlx_speculative/
black mlx_speculative/
```

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- **MLX Team** for the excellent MLX framework
- **mlx_parallm** for batched inference inspiration
- **vLLM** for speculative decoding reference implementation
- **Apple** for Apple Silicon and Metal Performance Shaders

## üìö Citation

If you use this work in your research, please cite:

```bibtex
@software{mlx_speculative_server,
  title={MLX Speculative Server: High-Performance LLM Serving for Apple Silicon},
  author={Your Name},
  year={2025},
  url={https://github.com/yourusername/mlx-speculative-server}
}
```

## üîó Related Projects

- [MLX](https://github.com/ml-explore/mlx) - Apple's machine learning framework
- [MLX-LM](https://github.com/ml-explore/mlx-lm) - Language models for MLX
- [mlx_parallm](https://github.com/willccbb/mlx_parallm) - Parallel inference for MLX
- [vLLM](https://github.com/vllm-project/vllm) - Fast LLM inference and serving

---

**Made with ‚ù§Ô∏è for Apple Silicon**
