# Copyright Â© 2025 Manus AI

import inspect
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

import mlx.core as mx
import mlx.nn as nn
from mlx_lm import load
from mlx_lm.tokenizer_utils import TokenizerWrapper


class BatchedKVCache:
    """
    Batched Key-Value cache for efficient parallel processing of multiple sequences.
    Adapted from mlx_parallm with enhancements for speculative decoding.
    """

    def __init__(self, head_dim: int, n_kv_heads: int, batch_size: int = 1, max_size: Optional[int] = None):
        """
        Initialize the batched KV cache.
        
        Args:
            head_dim: Dimension of each attention head
            n_kv_heads: Number of key-value heads
            batch_size: Number of sequences in the batch
            max_size: Maximum cache size (for memory management)
        """
        self.n_kv_heads = n_kv_heads
        self.head_dim = head_dim
        self.batch_size = batch_size
        self.max_size = max_size
        self.keys = None
        self.values = None
        self.offset = 0
        self.step = 256  # Growth step size

    def update_and_fetch(self, keys: mx.array, values: mx.array) -> Tuple[mx.array, mx.array]:
        """
        Update the cache with new keys and values and return the full cache.
        
        Args:
            keys: New keys to add [batch_size, n_kv_heads, seq_len, head_dim]
            values: New values to add [batch_size, n_kv_heads, seq_len, head_dim]
            
        Returns:
            Tuple of (all_keys, all_values) including the new entries
        """
        prev_offset = self.offset
        new_seq_len = keys.shape[2]
        
        # Check if we need to expand the cache
        if self.keys is None or (prev_offset + new_seq_len) > self.keys.shape[2]:
            self._expand_cache(keys, values, new_seq_len)
        
        # Update offset
        self.offset += new_seq_len
        
        # Store new keys and values
        self.keys[..., prev_offset:self.offset, :] = keys
        self.values[..., prev_offset:self.offset, :] = values
        
        # Return the active portion of the cache
        return self.keys[..., :self.offset, :], self.values[..., :self.offset, :]

    def _expand_cache(self, keys: mx.array, values: mx.array, new_seq_len: int):
        """Expand the cache to accommodate new entries."""
        # Calculate required size
        required_size = self.offset + new_seq_len
        
        # Apply max_size limit if specified
        if self.max_size and required_size > self.max_size:
            # Implement sliding window or other memory management strategy
            self._apply_memory_limit(required_size)
            return
        
        # Calculate expansion size
        n_steps = (self.step + required_size - 1) // self.step
        new_size = n_steps * self.step
        
        # Create new cache arrays
        shape = (self.batch_size, self.n_kv_heads, new_size, self.head_dim)
        new_keys = mx.zeros(shape, keys.dtype)
        new_values = mx.zeros(shape, values.dtype)
        
        # Copy existing data if any
        if self.keys is not None:
            copy_size = min(self.offset, new_size)
            new_keys[..., :copy_size, :] = self.keys[..., :copy_size, :]
            new_values[..., :copy_size, :] = self.values[..., :copy_size, :]
        
        self.keys = new_keys
        self.values = new_values

    def _apply_memory_limit(self, required_size: int):
        """Apply memory management when max_size is exceeded."""
        if self.max_size is None:
            return
        
        # Simple sliding window: keep the most recent tokens
        keep_size = self.max_size - (required_size - self.offset)
        if keep_size > 0 and self.offset > keep_size:
            # Shift the cache
            start_idx = self.offset - keep_size
            self.keys[..., :keep_size, :] = self.keys[..., start_idx:self.offset, :]
            self.values[..., :keep_size, :] = self.values[..., start_idx:self.offset, :]
            self.offset = keep_size

    def trim(self, num_tokens: int):
        """Trim the cache by removing the last num_tokens."""
        if num_tokens > 0 and num_tokens <= self.offset:
            self.offset -= num_tokens

    def reset(self):
        """Reset the cache to empty state."""
        self.offset = 0
        self.keys = None
        self.values = None

    @property
    def state(self) -> Dict[str, Any]:
        """Get the current state of the cache for evaluation."""
        return {
            "keys": self.keys[..., :self.offset, :] if self.keys is not None else None,
            "values": self.values[..., :self.offset, :] if self.values is not None else None,
            "offset": self.offset,
        }


def create_additive_causal_mask(N: int, offset: int = 0) -> mx.array:
    """Create an additive causal mask for attention."""
    rinds = mx.arange(offset + N)
    linds = mx.arange(offset, offset + N) if offset else rinds
    mask = linds[:, None] < rinds[None]
    return mask * -1e9


@dataclass
class ModelConfig:
    """Configuration for model loading and setup."""
    model_path: str
    draft_model_path: Optional[str] = None
    trust_remote_code: bool = False
    adapter_path: Optional[str] = None
    quantize: bool = False
    dtype: str = "float16"


def load_model_pair(
    config: ModelConfig,
    auto_draft: bool = True,
    draft_layers_ratio: float = 0.5,
) -> Tuple[nn.Module, nn.Module, TokenizerWrapper]:
    """
    Load a target model and its corresponding draft model.
    
    Args:
        config: Model configuration
        auto_draft: Whether to automatically create a draft model
        draft_layers_ratio: Ratio of layers to keep for auto-generated draft model
        
    Returns:
        Tuple of (target_model, draft_model, tokenizer)
    """
    # Load target model
    target_model, tokenizer = load(
        config.model_path,
        adapter_path=config.adapter_path,
        tokenizer_config={"trust_remote_code": config.trust_remote_code},
    )
    
    # Load or create draft model
    if config.draft_model_path:
        # Load explicit draft model
        draft_model, _ = load(
            config.draft_model_path,
            tokenizer_config={"trust_remote_code": config.trust_remote_code},
        )
    elif auto_draft:
        # Create draft model by layer pruning
        draft_model = create_draft_model(target_model, draft_layers_ratio)
    else:
        # Use target model as draft (no acceleration)
        draft_model = target_model
    
    return target_model, draft_model, tokenizer


def create_draft_model(target_model: nn.Module, layers_ratio: float = 0.5) -> nn.Module:
    """
    Create a draft model by pruning layers from the target model.
    
    Args:
        target_model: The target model to create a draft from
        layers_ratio: Ratio of layers to keep (0.5 = keep half the layers)
        
    Returns:
        Draft model with reduced layers
    """
    # This is a simplified implementation
    # In practice, you might want more sophisticated pruning strategies
    
    if not hasattr(target_model, 'layers'):
        # If model doesn't have layers attribute, return the same model
        return target_model
    
    num_layers = len(target_model.layers)
    keep_layers = max(1, int(num_layers * layers_ratio))
    
    # Create a copy of the model with fewer layers
    # This is a conceptual implementation - actual implementation would depend
    # on the specific model architecture
    
    # For now, return the same model
    # TODO: Implement actual layer pruning based on model architecture
    return target_model


def get_model_info(model: nn.Module) -> Dict[str, Any]:
    """Get information about a model's architecture."""
    info = {
        "num_parameters": sum(p.size for p in model.parameters()),
        "has_layers": hasattr(model, 'layers'),
        "num_layers": len(model.layers) if hasattr(model, 'layers') else 0,
        "head_dim": getattr(model, 'head_dim', None),
        "n_heads": getattr(model, 'n_heads', None),
        "n_kv_heads": getattr(model, 'n_kv_heads', None),
        "vocab_size": getattr(model, 'vocab_size', None),
    }
    
    return info


def estimate_memory_usage(model: nn.Module, batch_size: int = 1, seq_len: int = 2048) -> Dict[str, float]:
    """
    Estimate memory usage for a model with given batch size and sequence length.
    
    Returns:
        Dictionary with memory estimates in MB
    """
    # Get model parameters size
    param_size = sum(p.nbytes for p in model.parameters()) / (1024 * 1024)  # MB
    
    # Estimate KV cache size
    info = get_model_info(model)
    if info["num_layers"] > 0 and info["head_dim"] and info["n_kv_heads"]:
        # Each layer has keys and values
        # Size = batch_size * n_kv_heads * seq_len * head_dim * 2 (keys + values) * 2 (float16)
        kv_cache_size = (
            batch_size * info["n_kv_heads"] * seq_len * info["head_dim"] * 2 * 2 * info["num_layers"]
        ) / (1024 * 1024)  # MB
    else:
        kv_cache_size = 0
    
    # Estimate activation memory (rough approximation)
    activation_size = batch_size * seq_len * (info["vocab_size"] or 32000) * 2 / (1024 * 1024)  # MB
    
    return {
        "parameters": param_size,
        "kv_cache": kv_cache_size,
        "activations": activation_size,
        "total": param_size + kv_cache_size + activation_size,
    }
